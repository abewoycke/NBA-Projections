{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Projections Preprocessing and Modeling\n",
    "\n",
    "In this notebook, I build and tune three models (Gradient Boosting Classifier, Logistic Regression, and a Neural Network), and also the resulting soft voting classifier. A lot of the tuning to arrive at the selected hyperparameters was done on virtual machines or in other notebooks; this notebook contains the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"compare.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"home_win\"], axis=1)\n",
    "y = df.home_win.ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model One: Gradient Boosting Classifier\n",
    "Because GBC is computationally intensive, I use a RandomSearch for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for a grid or randomized search\n",
    "params = {\n",
    "    'loss':['exponential','deviance'],\n",
    "    'learning_rate':[0.05,0.1,0.2,0.3,0.4],\n",
    "    'n_estimators':[10,250,500],\n",
    "    'criterion':['friedman_mse', 'mse', 'mae'],\n",
    "    'min_samples_split':[1,5,10],\n",
    "    'min_samples_leaf':[1,4,8],\n",
    "    'min_weight_fraction_leaf':[0,0.05,0.1],\n",
    "    'max_depth':[2,3,4,7,8,9,None],\n",
    "    'min_impurity_decrease':[0,0.01,0.05],\n",
    "    'max_features':['sqrt','log2',8],\n",
    "    'warm_start':[True,False],\n",
    "    'n_iter_no_change':[25],\n",
    "    'ccp_alpha':[0,1000,2000]   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport warnings\\nwarnings.simplefilter(\"ignore\")\\ngbc = GradientBoostingClassifier()\\ngb_cv = RandomizedSearchCV(gbc, params, n_jobs=4, n_iter=25)\\ngb_cv.fit(X_train, y_train)\\nprint(gb_cv.best_params_)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I ran this code block separately on a VM with n_iter = 1000\n",
    "# Those results are included below\n",
    "'''\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "gbc = GradientBoostingClassifier()\n",
    "gb_cv = RandomizedSearchCV(gbc, params, n_jobs=4, n_iter=25)\n",
    "gb_cv.fit(X_train, y_train)\n",
    "print(gb_cv.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the above random search with n_iter=1000 showed the following parameters to be the best performing:\n",
    "\n",
    "GradientBoostingClassifier(loss='exponential', n_estimators=250,\n",
    "                                 min_samples_split=5, min_samples_leaf=4,\n",
    "                                 max_features=8).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom hyperopt import tpe,hp,fmin,STATUS_OK,Trials\\nfrom hyperopt.pyll.base import scope\\n\\nspace = {\\n    \\'loss\\': \\'exponential\\',\\n    \\'criterion\\': \\'mae\\',\\n    \\'learning_rate\\': hp.quniform(\"learning_rate\",0.01,0.5,0.01),\\n    \\'n_estimators\\': hp.choice(\\'n_estimators\\',np.arange(10, 1000, 50, dtype=int)),\\n    \\'min_samples_split\\':hp.choice(\"min_samples_split\",np.arange(2, 10, 1, dtype=int)),\\n    \\'min_samples_leaf\\': hp.choice(\"min_samples_leaf\",np.arange(2, 10, 1, dtype=int)),\\n    \\'min_weight_fraction_leaf\\':hp.choice(\"min_weight_fraction_leaf\",[0,0.05,0.1]),\\n    \\'max_depth\\': hp.uniform(\"max_depth\",1,12),\\n    \\'min_impurity_decrease\\':hp.choice(\"min_impurity_decrease\",[0,0.01,0.05]),\\n    \\'max_features\\':hp.choice(\"max_features\",[\\'sqrt\\',\\'log2\\',8]),\\n    \\'warm_start\\':hp.choice(\"warm_start\", [True,False]),\\n    \\'ccp_alpha\\':hp.uniform(\"ccp_alpha\",0,2500)\\n}\\n\\n# define objective function\\ndef objective(space):\\n    clf = GradientBoostingClassifier(loss= space[\\'loss\\'],\\n                                     learning_rate=space[\\'learning_rate\\'],\\n                                     n_estimators=space[\\'n_estimators\\'],\\n                                     criterion = space[\\'criterion\\'],\\n                                     min_samples_split=space[\\'min_samples_split\\'],\\n                                     min_samples_leaf=space[\\'min_samples_leaf\\'],\\n                                     min_weight_fraction_leaf=space[\\'min_weight_fraction_leaf\\'],\\n                                     max_depth=space[\\'max_depth\\'],\\n                                     min_impurity_decrease=space[\\'min_impurity_decrease\\'],\\n                                     max_features=space[\\'max_features\\'],\\n                                     warm_start=space[\\'warm_start\\'],\\n                                     ccp_alpha=space[\\'ccp_alpha\\'],\\n                                    )\\n\\n    clf.fit(X_train,y_train)\\n    acc = cross_val_score(clf, X_train, y_train, cv=5).mean()\\n    return{\\'loss\\':-acc, \\'status\\': STATUS_OK }\\n\\n# initialize trials object\\ntrials = Trials()\\n\\nbest = fmin(\\n    fn=objective,\\n    space=space,\\n    algo=tpe.suggest,\\n    max_evals=25,\\n    trials=trials\\n)\\n\\nprint(\"Best: {}\".format(best))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I ran this code block separately to use HyperOpt to tune\n",
    "# Those results are included below\n",
    "'''\n",
    "from hyperopt import tpe,hp,fmin,STATUS_OK,Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "space = {\n",
    "    'loss': 'exponential',\n",
    "    'criterion': 'mae',\n",
    "    'learning_rate': hp.quniform(\"learning_rate\",0.01,0.5,0.01),\n",
    "    'n_estimators': hp.choice('n_estimators',np.arange(10, 1000, 50, dtype=int)),\n",
    "    'min_samples_split':hp.choice(\"min_samples_split\",np.arange(2, 10, 1, dtype=int)),\n",
    "    'min_samples_leaf': hp.choice(\"min_samples_leaf\",np.arange(2, 10, 1, dtype=int)),\n",
    "    'min_weight_fraction_leaf':hp.choice(\"min_weight_fraction_leaf\",[0,0.05,0.1]),\n",
    "    'max_depth': hp.uniform(\"max_depth\",1,12),\n",
    "    'min_impurity_decrease':hp.choice(\"min_impurity_decrease\",[0,0.01,0.05]),\n",
    "    'max_features':hp.choice(\"max_features\",['sqrt','log2',8]),\n",
    "    'warm_start':hp.choice(\"warm_start\", [True,False]),\n",
    "    'ccp_alpha':hp.uniform(\"ccp_alpha\",0,2500)\n",
    "}\n",
    "\n",
    "# define objective function\n",
    "def objective(space):\n",
    "    clf = GradientBoostingClassifier(loss= space['loss'],\n",
    "                                     learning_rate=space['learning_rate'],\n",
    "                                     n_estimators=space['n_estimators'],\n",
    "                                     criterion = space['criterion'],\n",
    "                                     min_samples_split=space['min_samples_split'],\n",
    "                                     min_samples_leaf=space['min_samples_leaf'],\n",
    "                                     min_weight_fraction_leaf=space['min_weight_fraction_leaf'],\n",
    "                                     max_depth=space['max_depth'],\n",
    "                                     min_impurity_decrease=space['min_impurity_decrease'],\n",
    "                                     max_features=space['max_features'],\n",
    "                                     warm_start=space['warm_start'],\n",
    "                                     ccp_alpha=space['ccp_alpha'],\n",
    "                                    )\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    acc = cross_val_score(clf, X_train, y_train, cv=5).mean()\n",
    "    return{'loss':-acc, 'status': STATUS_OK }\n",
    "\n",
    "# initialize trials object\n",
    "trials = Trials()\n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=25,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best: {}\".format(best))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results from the HyperOpt search:\n",
    "\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.43,\n",
    "                                     n_estimators=10,\n",
    "                                     criterion='mse',\n",
    "                                     min_samples_split=6,\n",
    "                                     min_samples_leaf=6,\n",
    "                                     min_weight_fraction_leaf=0.1,\n",
    "                                     max_depth=8,\n",
    "                                     max_features=None,\n",
    "                                     warm_start=False,\n",
    "                                     ccp_alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a narrowed GridSearch to see if we can eke out any more improvement\n",
    "params = {\n",
    "    'loss':['exponential'],\n",
    "    'learning_rate':[0.05,0.1,0.2],\n",
    "    'n_estimators':[250],\n",
    "    'criterion':['friedman_mse', 'mse', 'mae'],\n",
    "    'min_samples_split':[4,5,6],\n",
    "    'min_samples_leaf':[3,4,5],\n",
    "    'max_depth':[2,3,None],\n",
    "    'max_features':[8],\n",
    "    'warm_start':[True,False],\n",
    "    'n_iter_no_change':[25], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# execute narrowed GridSearch\\nwarnings.simplefilter(\"ignore\")\\ngbc = GradientBoostingClassifier()\\ngb_cv = GridSearchCV(gbc, params, n_jobs=7)\\ngb_cv.fit(X_train, y_train)\\nprint(gb_cv.best_params_)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# execute narrowed GridSearch\n",
    "warnings.simplefilter(\"ignore\")\n",
    "gbc = GradientBoostingClassifier()\n",
    "gb_cv = GridSearchCV(gbc, params, n_jobs=7)\n",
    "gb_cv.fit(X_train, y_train)\n",
    "print(gb_cv.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results from the narowed GridSeach:\n",
    "{'criterion': 'mse', 'learning_rate': 0.1,\n",
    " 'loss': 'exponential', 'max_depth': 2,\n",
    " 'max_features': 8, 'min_samples_leaf': 4,\n",
    " 'min_samples_split': 6, 'n_estimators': 250, \n",
    " 'n_iter_no_change': 25, 'warm_start': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search's Tuned AUROC= 0.695\n",
      "Random Search's Tuned Accuracy = 0.658\n",
      "HyperOpt's Tuned AUROC= 0.696\n",
      "HyperOpt's Tuned Accuracy = 0.663\n",
      "GridSearch's Tuned AUROC= 0.698\n",
      "GridSearch's Tuned Accuracy = 0.661\n"
     ]
    }
   ],
   "source": [
    "# check AUROCs and accuracys for recommended models\n",
    "random_search = GradientBoostingClassifier(loss='exponential', n_estimators=250,\n",
    "                                 min_samples_split=5, min_samples_leaf=4,\n",
    "                                 max_features=8).fit(X_train,y_train)\n",
    "y_pred = random_search.predict_proba(X_test)\n",
    "y_pred = pd.DataFrame(y_pred,columns=['Loss','Win']).drop(columns=['Loss'])\n",
    "print(\"Random Search's Tuned AUROC= \" + str(round(roc_auc_score(y_test,y_pred),3)))\n",
    "y_pred = random_search.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred, columns=['home_win_prob'])\n",
    "predictions['binary'] = predictions['home_win_prob'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"Random Search's Tuned Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))\n",
    "\n",
    "HyperOpt = GradientBoostingClassifier(learning_rate=0.43, n_estimators=10,\n",
    "                                     criterion='mse', min_samples_split=6,\n",
    "                                     min_samples_leaf=6, min_weight_fraction_leaf=0.1,\n",
    "                                     max_depth=8, max_features=None,\n",
    "                                     warm_start=False, ccp_alpha=0).fit(X_train,y_train)\n",
    "\n",
    "y_pred = HyperOpt.predict_proba(X_test)\n",
    "y_pred = pd.DataFrame(y_pred,columns=['Loss','Win']).drop(columns=['Loss'])\n",
    "print(\"HyperOpt's Tuned AUROC= \" + str(round(roc_auc_score(y_test,y_pred),3)))\n",
    "y_pred = HyperOpt.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred, columns=['home_win_prob'])\n",
    "predictions['binary'] = predictions['home_win_prob'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"HyperOpt's Tuned Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))\n",
    "\n",
    "partial_grid = GradientBoostingClassifier(criterion='mse',loss='exponential',\n",
    "                                          max_depth=2,max_features=8,\n",
    "                                         min_samples_leaf=4, min_samples_split=6,\n",
    "                                         n_estimators=250, warm_start=True).fit(X_train,y_train)\n",
    "y_pred = partial_grid.predict_proba(X_test)\n",
    "y_pred = pd.DataFrame(y_pred,columns=['Loss','Win']).drop(columns=['Loss'])\n",
    "print(\"GridSearch's Tuned AUROC= \" + str(round(roc_auc_score(y_test,y_pred),3)))\n",
    "y_pred = partial_grid.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred, columns=['home_win_prob'])\n",
    "predictions['binary'] = predictions['home_win_prob'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"GridSearch's Tuned Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after the above tuning, the best parameters seem *barely* to be those arrived at by our narrowed GridSearch.\n",
    "\n",
    "partial_grid = GradientBoostingClassifier(criterion='mse',loss='exponential',\n",
    "                                          max_depth=2,max_features=8,\n",
    "                                         min_samples_leaf=4, min_samples_split=6,\n",
    "                                         n_estimators=250, warm_start=True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GradientBoostingClassifier(criterion='mse',loss='exponential',\n",
    "                                          max_depth=2,max_features=8,\n",
    "                                         min_samples_leaf=4, min_samples_split=6,\n",
    "                                         n_estimators=250, warm_start=True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Two: Logistic Regression\n",
    "Because Logit is not computationally intensive, I use a GridSearch for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set param grid for Logit GridSearch\n",
    "params = {\n",
    "    'penalty':['l1','l2','elasticnet','none'],\n",
    "    'fit_intercept':[True,False],\n",
    "    'solver':['newton-cg','lbfgs','liblinear','sag','saga'],\n",
    "    'warm_start':[True,False],\n",
    "    'n_jobs':[6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_intercept': False, 'n_jobs': 6, 'penalty': 'l1', 'solver': 'liblinear', 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# run a GridSearch here\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logit = LogisticRegression()\n",
    "logit_cv = GridSearchCV(logit, param_grid=params, scoring=\"roc_auc\", n_jobs=6)\n",
    "logit_cv.fit(X_train, y_train)\n",
    "print(logit_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logit's AUROC= 0.696\n",
      "Tuned Logit's Accuracy = 0.643\n"
     ]
    }
   ],
   "source": [
    "# check AUROC for model using best params from the GridSearch\n",
    "clf2 = LogisticRegression(penalty='l1',solver='liblinear',warm_start=False,fit_intercept=False).fit(X_train,y_train)\n",
    "y_pred = clf2.predict_proba(X_test)\n",
    "y_pred = pd.DataFrame(y_pred,columns=['Loss','Win']).drop(columns=['Loss'])\n",
    "print(\"Tuned Logit's AUROC= \" + str(round(roc_auc_score(y_test,y_pred),3)))\n",
    "\n",
    "# check accuracy\n",
    "y_pred = clf2.predict(X_test)\n",
    "predictions = pd.DataFrame(y_pred, columns=['home_win_prob'])\n",
    "predictions['binary'] = predictions['home_win_prob'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"Tuned Logit's Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Three: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust format of inputs for neural network classifier\n",
    "predictors = np.matrix(X_train)\n",
    "target = to_categorical(y_train)\n",
    "X_val = np.matrix(X_test)\n",
    "y_val = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping callback to expedite tuning search\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", min_delta=1e-3,patience=3,verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create + fit model, required for KerasClassifier\n",
    "\n",
    "def create_model(layers,nodes,activation,batch_size,optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(14,)))\n",
    "    for x in range(layers):\n",
    "        model.add(Dense(nodes, activation=activation))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(predictors,target,batch_size=batch_size,epochs=50,validation_data=(X_val, y_val),callbacks=callbacks,verbose=0)\n",
    "    model._estimator_type = 'regressor'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set param grid for NN GridSearch\n",
    "\n",
    "layers= [2,4,8,16]\n",
    "nodes= [32,64,128,256]\n",
    "activation = ['relu','sigmoid','softmax','softplus','softsign','tanh','selu','elu','exponential']\n",
    "batch_size = [10,25,50,75,100]\n",
    "param_grid = dict(layers=layers,nodes=nodes,activation=activation,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngrid = GridSearchCV(estimator=model,param_grid=param_grid,cv=5)\\ngrid_result=grid.fit(X_train,y_train)\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\nstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\nfor mean, stdev, param in zip(means, stds, params):\\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the GridSearch (done in another notebook)\n",
    "'''\n",
    "grid = GridSearchCV(estimator=model,param_grid=param_grid,cv=5)\n",
    "grid_result=grid.fit(X_train,y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First GridSearch's Optimal Parameters for NN__\n",
    "Layers: 16\n",
    "Nodes: 128\n",
    "Activation: 'relu'\n",
    "Batch Size: 64\n",
    "\n",
    "I ran an additional GridSearch including different optimizers after that one out of curiosity and ended up with this set of parameters that might be very good as well:\n",
    "\n",
    "__Second GridSearch's Optimal Parameters for NN__\n",
    "Layers: 8\n",
    "Nodes: 70\n",
    "Activation: 'relu'\n",
    "Batch Size: 100\n",
    "Optimizer: 'RMSprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Tuned Neural Net's AUROC= 0.691\n",
      "First Tuned Neural Net's Accuracy = 0.656\n",
      "Second Tuned Neural Net's AUROC= 0.689\n",
      "Second Tuned Neural Net's Accuracy = 0.659\n"
     ]
    }
   ],
   "source": [
    "# check AUROCs and Accuracies of Two Tuned NNs\n",
    "\n",
    "NN1 = create_model(16,128,'relu',64,'adam')\n",
    "y_pred = NN1.predict(X_val)\n",
    "y_probs = pd.DataFrame(y_pred,columns=['Loss','Win']).drop(columns=['Loss'])\n",
    "print(\"First Tuned Neural Net's AUROC= \" + str(round(roc_auc_score(y_test,y_probs),3)))\n",
    "# check accuracy\n",
    "predictions = pd.DataFrame(y_pred, columns=['home_loss_prob','home_win_prob'])\n",
    "predictions['binary'] = predictions['home_win_prob'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"First Tuned Neural Net's Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))\n",
    "\n",
    "# second NN\n",
    "\n",
    "NN2 = create_model(8,70,'relu',100,'RMSprop')\n",
    "\n",
    "y_pred = NN2.predict(X_val)\n",
    "y_probs = pd.DataFrame(y_pred,columns=['Loss','Win']).drop(columns=['Loss'])\n",
    "print(\"Second Tuned Neural Net's AUROC= \" + str(round(roc_auc_score(y_test,y_probs),3)))\n",
    "# check accuracy\n",
    "predictions = pd.DataFrame(y_pred, columns=['home_loss_prob','home_win_prob'])\n",
    "predictions['binary'] = predictions['home_win_prob'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"Second Tuned Neural Net's Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first neural net is slightly better on AUROC, which I defined before as my primary performance metric. So I'll opt for that one, even with the slightly lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = create_model(16,128,'relu',64,'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Four + Five: Soft and Hard Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I had trouble instantiating a formal sklearn VotingClassifier with the Neural Network sklearn wrapper KerasClassifier, I wrote some user-defined functions to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_voting(input,*args):\n",
    "    '''takes an input array and fitted predictors, and returns an array of their predictions averaged'''\n",
    "    preds = {}\n",
    "    clfs=[]\n",
    "    vote_preds = []\n",
    "    for arg in args:\n",
    "        if arg._estimator_type == 'classifier':\n",
    "            pred = arg.predict_proba(input)\n",
    "            preds[arg] = pred\n",
    "        else:\n",
    "            new_preds = []\n",
    "            pred = arg.predict(input)\n",
    "            for entry in pred:\n",
    "                new_preds.append(entry[1])\n",
    "            preds[arg] = new_preds\n",
    "    for key in preds.keys():\n",
    "        clfs.append(key)\n",
    "    for x in range(len(input)):\n",
    "        votes = []\n",
    "        for clf in clfs:\n",
    "            votes.append(preds[clf][x])\n",
    "        vote_preds.append(np.mean(votes))\n",
    "    return vote_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_voting(input,*args):\n",
    "    '''takes an input array and fitted predictors, and returns an array of their prediction consensus'''\n",
    "    preds = {}\n",
    "    clfs=[]\n",
    "    vote_preds = []\n",
    "    for arg in args:\n",
    "        if arg._estimator_type == 'classifier':\n",
    "            pred = arg.predict(input)\n",
    "            preds[arg] = pred\n",
    "        else:\n",
    "            new_preds = []\n",
    "            pred = (arg.predict(input) > 0.5).astype(\"int32\")\n",
    "            for entry in pred:\n",
    "                new_preds.append(entry[1])\n",
    "            preds[arg] = new_preds\n",
    "    for key in preds.keys():\n",
    "        clfs.append(key)\n",
    "    for x in range(len(input)):\n",
    "        votes = []\n",
    "        for clf in clfs:\n",
    "            votes.append(preds[clf][x])\n",
    "        vote_preds.append(np.mean(votes))\n",
    "    return vote_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting's AUROC*= 0.67\n",
      "Hard Voting's Accuracy = 0.657\n"
     ]
    }
   ],
   "source": [
    "# hard voting's Accuracy\n",
    "y_pred = hard_voting(X_val,clf1,clf2,NN)\n",
    "print(\"Hard Voting's AUROC*= \" + str(round(roc_auc_score(y_test,y_pred),3)))\n",
    "predictions = pd.DataFrame(y_pred, columns=['home_win_prob'])\n",
    "predictions['binary'] = predictions['home_win_prob'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"Hard Voting's Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Because hard voting in this custom-made format doesn't really have predicted probabilities as an output, using AUROC as a metric here is a bit uninformative. I've included it only for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting's AUROC= 0.697\n",
      "Soft Voting's Accuracy = 0.662\n"
     ]
    }
   ],
   "source": [
    "# soft voting's AUROC\n",
    "y_pred = soft_voting(X_val,clf1,clf2,NN)\n",
    "y_pred = pd.DataFrame(y_pred,columns=['Loss','Win']).drop(columns=['Loss'])\n",
    "print(\"Soft Voting's AUROC= \" + str(round(roc_auc_score(y_test,y_pred),3)))\n",
    "# soft voting's Accuracy\n",
    "y_pred = soft_voting(X_val,clf1,clf2,NN)\n",
    "predictions = pd.DataFrame(y_pred, columns=['Loss','Win'])\n",
    "predictions['binary'] = predictions['Win'] > 0.5\n",
    "y_pred = np.array(predictions['binary'])\n",
    "print(\"Soft Voting's Accuracy = \" + str(round(accuracy_score(y_test,y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The best performing of the three individual models by AUROC is __GradientBoostingClassifier__, with an AUROC of __0.698__.\n",
    "The best performing of the three individual models by classification accuracy is also __GradientBoostingClassifier__, with an accuracy of __0.661__.\n",
    "\n",
    "The voting classifiers, by comparison, had:\n",
    "Soft: an AUROC of __0.697__ and an accuracy of __0.662__.\n",
    "Hard: an AUROC* (see note above) of __0.67__ and an accuracy of __0.657__.\n",
    "\n",
    "For the purpose of most accurately predicting game outcomes, I would probably opt to use the __GradientBoostingClassifier__. The soft voting classifiier might also be worth considering. All of these differences in model performance metrics are very small, and are also influnced by the random chance from the selection of our sample. Even more hyperparameter tuning could perhaps find an incrementally better model. I'm curious to see how the models performs on fresh data, like the upcoming 2020-2021 NBA season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(criterion='mse',loss='exponential',\n",
    "                                          max_depth=2,max_features=8,\n",
    "                                         min_samples_leaf=4, min_samples_split=6,\n",
    "                                         n_estimators=250, warm_start=True).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
